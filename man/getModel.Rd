% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/getModel.R
\name{getModel}
\alias{getModel}
\title{Smoothed Quantile Regression with Interquantile Shrinkage (Stan)}
\usage{
getModel(
  y,
  taus,
  H = NULL,
  X = NULL,
  offset = NULL,
  w = 0,
  alpha = 0.75,
  eps_w = 0.001,
  c_sigma = 1,
  beta_sd = 1,
  lambda_nc = 2,
  T_rel = 0.1,
  adaptive_iq = TRUE,
  lambda_iq2_a = 1,
  lambda_iq2_b = 0.1,
  lambda_iq2_fixed = 1,
  adaptive_gamma = TRUE,
  lambda_lasso2_a = 1,
  lambda_lasso2_b = 0.05,
  lambda_lasso2_fixed = 1,
  log_flag = 0,
  jittering = 0,
  chains = 1,
  iter = 1500,
  warmup = 500,
  control = list(adapt_delta = 0.99),
  seed = 123,
  verbose = FALSE,
  map_hessian = TRUE,
  map_tol_obj = 1e-12,
  map_tol_grad = 1e-08,
  map_tol_rel_grad = 10000,
  map_tol_param = 1e-08,
  map_iter = 2000,
  fit_method = c("mcmc", "map_mcmc", "map"),
  laplace_n_samples = 1000,
  laplace_noise_scale = 0.1,
  prior_gamma = c("group_lasso", "lasso", "spike_slab", "het_group_lasso",
    "adaptive_lasso"),
  spike_sd = 0.05,
  slab_sd = 2,
  slab_pi_a = 1,
  slab_pi_b = 1
)
}
\arguments{
\item{y}{Numeric vector of responses of length \eqn{n}.}

\item{taus}{Numeric vector of target quantile levels in \eqn{(0,1)}, length \eqn{m}.}

\item{H}{Numeric matrix \eqn{n \times r} of structured predictors for group-lasso
coefficients \eqn{\gamma}. If \eqn{r = 0}, pass a zero-column matrix.}

\item{X}{Optional numeric matrix \eqn{n \times p_x} of additional predictors.}

\item{offset}{Optional numeric vector of length \eqn{n} added to the linear predictor.}

\item{w}{Integer \eqn{\ge 1}. Used for initial quantile estimation from first w observations.}

\item{beta_sd}{Positive scalar prior std dev for \code{beta} coefficients (default 1.0).}

\item{lambda_nc}{Positive scalar weight for the non-crossing penalty (larger is stricter).}

\item{T_rel}{Positive scalar "smoothing temperature" (dimensionless).}

\item{adaptive_iq}{Logical; if TRUE (default), the IQ shrinkage rate lambda_iq2
is learned from data via a Gamma prior. If FALSE, lambda_iq2_fixed is used.}

\item{lambda_iq2_a, lambda_iq2_b}{Positive shape/rate hyperparameters for the
IQ shrinkage rate \eqn{\lambda_{iq}^2} (used when adaptive_iq = TRUE).
Prior: \eqn{\lambda_{iq}^2 \sim \mathrm{Gamma}(a, b)}, mean = a/b.
Effective penalty weight is \eqn{\sqrt{\lambda_{iq}^2}}.}

\item{lambda_iq2_fixed}{Positive scalar; fixed value for \eqn{\lambda_{iq}^2} when
adaptive_iq = FALSE (default 1). Effective penalty = \eqn{\sqrt{\lambda_{iq2\_fixed}}}.}

\item{adaptive_gamma}{Logical; if TRUE (default), the global LASSO rate lambda_lasso2
is learned from data via a Gamma prior. If FALSE, lambda_lasso2_fixed is used as a fixed value.}

\item{lambda_lasso2_a, lambda_lasso2_b}{Positive shape/rate hyperparameters for the
global LASSO rate \eqn{\lambda} (used when adaptive_gamma = TRUE).}

\item{lambda_lasso2_fixed}{Positive scalar; fixed value for global LASSO rate when
adaptive_gamma = FALSE (default 1).}

\item{log_flag}{Integer \code{0/1}. If 1, fit on \code{log(y)}.}

\item{jittering}{Integer \code{0/1}. If 1, add \eqn{u \sim \mathrm{Beta}(1,1)} to \eqn{y}.}

\item{chains}{Number of MCMC chains.}

\item{iter}{Total iterations per chain.}

\item{warmup}{Warmup iterations per chain.}

\item{control}{Optional list passed to \code{rstan::sampling()}.}

\item{seed}{RNG seed.}

\item{verbose}{show the log.}

\item{map_hessian}{Logical; if \code{TRUE} compute Hessian in MAP step.}

\item{map_tol_obj, map_tol_grad, map_tol_rel_grad, map_tol_param}{MAP optimizer tolerances.}

\item{map_iter}{Maximum iterations for MAP optimization.}

\item{fit_method}{One of "mcmc", "map_mcmc", or "map":
\itemize{
  \item "mcmc": Estimators are posterior median from MCMC; posterior draws from MCMC.
  \item "map_mcmc": Estimators are MAP; posterior draws from MCMC (MAP used as init).
  \item "map": Estimators are MAP; posterior draws from Laplacian approximation.
}}

\item{laplace_n_samples}{Number of samples for Laplacian approximation (when fit_method = "map").}

\item{laplace_noise_scale}{Scale factor for parameter perturbation in Laplacian approximation.}

\item{prior_gamma}{Prior type for gamma: "group_lasso", "lasso", "spike_slab", "het_group_lasso", "adaptive_lasso".}

\item{spike_sd, slab_sd, slab_pi_a, slab_pi_b}{Spike-and-slab hyperparameters.}
}
\value{
A list with components:
  \itemize{
    \item fit: stanfit object (NULL if fit_method = "map")
    \item map: MAP estimates (contains $par with parameter values)
    \item y, H, X: Input data and design matrices
    \item hessian: Hessian at MAP (if computed)
    \item fit_method: The estimation method used
    \item laplace_samples: Pre-generated Laplacian samples (if fit_method = "map")
  }
}
\description{
Fits a multi-quantile (\eqn{m}) regression model where the conditional
quantile function is modeled as a latent random walk in time (or index)
with optional fixed effects \eqn{X} and structured effects \eqn{H}.
The \eqn{H}-coefficients are shrunk via a **grouped Bayesian LASSO**
(column-wise sharing across quantiles), and adjacent quantiles are softly
penalized to discourage crossings. **Interquantile shrinkage** stabilizes
outer quantiles by penalizing differences between adjacent quantile coefficients.
}
\section{Model (high level)}{

\describe{
  \item{Data & design}{
    \itemize{
      \item \eqn{y_i} is optionally jittered (\code{u ~ Beta(1,1)}) and/or log-transformed.
      \item Combined linear predictor \eqn{\eta_{qi} = \mu_{q,i} + x_i^\top \beta_q + h_i^\top \gamma_q + \mathrm{offset}_i}.
      \item \eqn{\mu_{q,\cdot}} is a random walk per quantile: \eqn{\mu_{q,t} = \mu_{q,t-1} + \tau^{(rw)}_q z_{q,t-1}}.
    }
  }
  \item{Interquantile shrinkage}{
    Penalizes differences between adjacent quantile coefficients for gamma and beta slopes:
    \eqn{\text{pen}_{\text{IQ}} = \sum_{q=2}^m w_q (|\gamma_{q} - \gamma_{q-1}| + |\beta_{q}^{slope} - \beta_{q-1}^{slope}|)}
    where \eqn{w_q = 1/\sqrt{\tau_q(1-\tau_q)\tau_{q-1}(1-\tau_{q-1})}} gives more weight to outer quantiles.
    This stabilizes outer quantiles by borrowing strength from inner quantiles.
    Note: Intercept is NOT penalized (per Jiang, Wang, & Bondell 2013).
    Note: mu (random walk) is NOT penalized to allow quantile-specific temporal evolution.
  }
  \item{Non-crossing penalty}{
    Adds an L1 hinge on the finite-difference derivative in \eqn{\tau},
    scaled by \code{lambda_nc}.
  }
}
}

\references{
Jiang, L., Wang, H. J., & Bondell, H. D. (2013). Interquantile Shrinkage in Regression Models.
Journal of Computational and Graphical Statistics, 22(4), 970-986.
}
